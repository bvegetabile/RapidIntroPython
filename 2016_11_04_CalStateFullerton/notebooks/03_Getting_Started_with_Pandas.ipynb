{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Rapid and Informal Introduction to Python for Data Science\n",
    "\n",
    "# - Pandas Fundamentals\n",
    "\n",
    "#### Developed by:  Brian Vegetabile, PhD Candidate, University of California, Irvine\n",
    "\n",
    "This notebook is a supplement to the workshop \"A Rapid and Informal Introduction to Python for Data Science\"\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "Import `numpy` as `np` and then import `pandas` as `pd` to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas DataFrame\n",
    "\n",
    "Numpy arrays are a fantastic way to start to perform numerical computations in Python, but they aren't often the way that we think about data as data scientists.  We often think about data as 2-dimensional tables where each column represents another a that we are be interested in.  This is one of the reasons why Microsoft Excel is still so prevalent for looking at data.  It clearly and concisely (okay this point may be argued) organizes data for the user.  The `R` language mimics this structure with their dataframe objects. \n",
    "\n",
    "To emulate this data structure in Python, the `pandas` library provides the `DataFrame` object which is built upon `numpy.array` objects. Let's jump right into playing with some data and examine the Pandas `DataFrame` structure.  \n",
    "\n",
    "Let's read the iris data set into the notebook.\n",
    "\n",
    "```python\n",
    "iris = pd.read_csv('data/iris.data')\n",
    "iris.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iris = pd.read_csv('../data/iris.data')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already shown two great commands in pandas.  First the `pd.read_csv()` function.  The primary reason that we didn't get into file `i/o` in standard Python is because some of the packages for data science provide great functionality already.  The second command is the `DataFrame.head()` command.  This is similar to UNIX `head` command which reads the first few lines of a document.  Pandas comes fully stocked with commands to summarize DataFrames. \n",
    "\n",
    "Additionally, notice that pandas \"just works\" with the jupyter notebook.  Without having to type `print` as the last command, the notebook put the table into a nice formatting.  Pandas DataFrames are set up nicely to look pretty within Jupyter notebooks.\n",
    "\n",
    "Typing `iris.<tab>` we see that we have access to over a few hundred different types of commands!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing DataFrames\n",
    "\n",
    "One of the first things that I like to do when beginning to examine data is to do a quick summary of all of the data types.  If I were looking at this in `R` for example, I would use the `summary` function.  The command in pandas though is `DataFrame.describe()`\n",
    "\n",
    "```python\n",
    "iris.describe()\n",
    "```\n",
    "\n",
    "This provides the count, mean and standard deviation as well as the five number summary of all numerical data types.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access specific columns of the DataFrame we can do so by using dot notation or by indexing by the name\n",
    "\n",
    "```python\n",
    "iris.SepalLength\n",
    "iris['SepalLength']\n",
    "```\n",
    "\n",
    "And we can do summaries on these as well\n",
    "\n",
    "```python\n",
    "iris['SepalLength'].describe()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access multiple columns by name, it is as simple as putting them within a list within brackets following the name of the dataset\n",
    "\n",
    "```python\n",
    "iris[['SepalLength', 'SepalWidth']].head()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris[['SepalLength', 'SepalWidth']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, pandas provides some simple statistical functionality as well such as finding the pairwise correlation of different columns of the dataframe\n",
    "\n",
    "```python\n",
    "iris[['SepalLength', 'SepalWidth']].corr()\n",
    "iris[[0,1,2,3]].corr()\n",
    "iris[range(0,4)].corr()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of functionality provided for simple summary statistics that you'd want transitioning from `R` or `MATLAB`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting Data\n",
    "\n",
    "Let's consider the `iris` data for a second.  We notice that the last column contains categorical variables.  Specifically the names of the specific plants.  Let's find the unique values of the this variable.\n",
    "\n",
    "```python\n",
    "unique_iris = iris.IrisType.unique\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are three unique types of Iris flowers.  Now let's create a mask a which will pull out the first only the iris data with the first unique iris type.  \n",
    "\n",
    "```python\n",
    "iris_mask = iris.IrisType == unique_iris[0]\n",
    "print iris_mask\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it is an array full of `True` and `False` statements. To use this in against our `iris` DataFrame we use the following notation.\n",
    "\n",
    "```python\n",
    "iris[iris_mask]\n",
    "```\n",
    "\n",
    "This mask operates on the rows of the DataFrame by convention.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Datasets\n",
    "\n",
    "A last big functionality that is worth mentioning before moving on is the ability of Pandas to manage data across multiple datasets.  We'll use this dataset throughout the day for a few examples. The original dataset consists of 337 observations and 18 variables from the set of Major League Baseball players who played at least one game in both the 1991 and 1992 seasons, excluding pitchers. The dataset contains the 1992 salaries for that population, along with performance measures for each player. Four categorical variables indicate how free each player was to move to other teams.\n",
    "\n",
    "The original dataset was split into two files for our purposes today, for the original see the reference\n",
    "- Pay for Play: Are Baseball Salaries Based on Performance?\n",
    "    http://www.amstat.org/publications/jse/v6n2/datasets.watnik.html\n",
    "\n",
    "Read in the following data:\n",
    "\n",
    "```python\n",
    "playerInfo = pd.read_csv('data/playerInfo.csv')\n",
    "performanceInfo = pd.read_csv('data/performanceInfo.csv)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "playerInfo = pd.read_csv('../data/playerInfo.csv')\n",
    "perfInfo = pd.read_csv('../data/performanceInfo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a quick `head` of the data we see that some rows from the `playerInfo` data may not be within the `performanceInfo` file.  This means that we will need to merge these two datasets.  Fortunately, pandas has very nice functionality for this as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print playerInfo.head()\n",
    "print perfInfo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining datasets\n",
    "\n",
    "From the above we see that both datasets have the common column `PlayerID`, but now let's check which DataFrame has more records.\n",
    "\n",
    "```python\n",
    "print playerInfo.shape\n",
    "print performaceInfo.shape\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print playerInfo.shape\n",
    "print perfInfo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are more players than there are performace data lines. Let's attempt to merge these two files together.\n",
    "\n",
    "To do this we use the `DataFrame.merge()` function.  \n",
    "\n",
    "```python\n",
    "DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False)\n",
    "```\n",
    "Merge DataFrame objects by performing a database-style join operation by columns or indexes.\n",
    "```python\n",
    "    Parameters\n",
    "    ----------\n",
    "    right : DataFrame\n",
    "    how : {'left', 'right', 'outer', 'inner'}, default 'inner'\n",
    "        * left: use only keys from left frame (SQL: left outer join)\n",
    "        * right: use only keys from right frame (SQL: right outer join)\n",
    "        * outer: use union of keys from both frames (SQL: full outer join)\n",
    "        * inner: use intersection of keys from both frames (SQL: inner join)\n",
    "    on : label or list\n",
    "        Field names to join on. Must be found in both DataFrames. If on is\n",
    "        None and not merging on indexes, then it merges on the intersection of\n",
    "        the columns by default.\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    sort : boolean, default False\n",
    "        Sort the join keys lexicographically in the result DataFrame\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "```\n",
    "\n",
    "Let's construct a Pandas merge to join the two datafiles.\n",
    "\n",
    "```python\n",
    "baseball = playerInfo.merge(performanceInfo, on='PlayerID', how='outer', sort=True)\n",
    "```\n",
    "\n",
    "This merges on the common identifier of `PlayerID` performing an `outer` merge so that it returns all of the data and then we sort the data on that key so that we can see the newly entered individuals. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "playerInfo.merge(perfInfo, on='PlayerID', how='outer', sort=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to files\n",
    "\n",
    "Pandas also provides functionality for writing back out to files. Now that we've constructed a cleaned dataset with all of the information we might want to write this to a file. \n",
    "\n",
    "Let's write the DataFrame `baseball` to a file using the `DataFrame.to_csv()` function.  \n",
    "\n",
    "```python\n",
    "baseball.to_csv('data/baseball.csv', index=False)\n",
    "```\n",
    "\n",
    "The `index=False` command ensures that we don't write the row names to the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should be enough to get you started with manipulating data in python.  Now let's move onto the next worksheet focused on visualizing data.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
